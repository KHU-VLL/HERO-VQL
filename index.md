---
layout: project_page
permalink: /

title_header: "HERO-VQL"
title: "HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization"
authors:
    - '<a href="https://joohyunchang.com/" target="_blank">Joohyun Chang<sup>1*</sup></a>'
    - '<a href="https://github.com/soyeonhong/" target="_blank">Soyeon Hong<sup>1*</sup></a>'
    - '<a href="https://sites.google.com/view/hyogun-lee/" target="_blank">Hyogun Lee<sup>1*</sup></a>'
    - '<a href="https://www.linkedin.com/in/seongjongha/" target="_blank">Seong Jong Ha<sup>2</sup></a>'
    - '<a href="https://github.com/potatowarriors/" target="_blank">Dongho Lee<sup>2</sup></a>'
    - '<a href="https://ami.khu.ac.kr/people/pi/" target="_blank">Seong Tae Kim<sup>1â€ </sup></a>'
    - '<a href="https://sites.google.com/site/jchoivision/" target="_blank">Jinwoo Choi<sup>1â€ </sup></a>'
affiliations:
    - <sup>1</sup>Kyung Hee University
    - <sup>2</sup>AI R&D Division, CJ Group
# paper: https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf
arxiv: https://arxiv.org/abs/2509.00385
# video: https://www.youtube.com/results?search_query=turing+machine
code: https://github.com/KHU-VLL/HERO-VQL
# data: https://huggingface.co/docs/datasets
highlight: "ðŸ”¥ BMVC 2025 Oral Paper"
footnote: "* Equally contributed first authors. â€  Corresponding author."
---

<!-- Using HTML to center the abstract -->
<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Abstract</h2>
        <div class="content has-text-justified">
In this work, we tackle the egocentric visual query localization (VQL), where a model should localize the query object in a long-form egocentric video. Frequent and abrupt viewpoint changes in egocentric videos cause significant object appearance variations and partial occlusions, making it difficult for existing methods to achieve accurate localization. To tackle these challenges, we introduce Hierarchical, Egocentric and RObust Visual Query Localization (HERO-VQL), a novel method inspired by human cognitive process in object recognition. We propose i) Top-down Attention Guidance (TAG) and ii) Egocentric Augmentation based Consistency Training (EgoACT). Top-down Attention Guidance refines the attention mechanism by leveraging the class token for high-level context and principal component score maps for fine-grained localization. To enhance learning in diverse and challenging matching scenarios, EgoAug enhances query diversity by replacing the query with a randomly selected corresponding object from ground-truth annotations and simulates extreme viewpoint changes by reordering video frames. Additionally, CT loss enforces stable object localization across different augmentation scenarios. Extensive experiments on VQ2D dataset validate that HERO-VQL effectively handles egocentric challenges, significantly outperforming baselines.
        </div>
    </div>
</div>

---

<!-- > Note: This is an example of a Jekyll-based project website template: [Github link](https://github.com/shunzh/project_website).\
> The following content is generated by ChatGPT. The figure is manually added. -->


<!-- ## Architecture -->
<div align="center"><h2> Problem </h2></div>

![Teaser](/static/image/Teaser.png)
(a) Given an egocentric video and a query image of an object, the goal is to localize the last occurrence of the query object in the video. (b) Unlike third-person videos, egocentric videos undergo abrupt viewpoint changes due to the camera wearerâ€™s movements. (c) These viewpoint changes introduce significant challenges in VQL, including variations in object appearance across perspectives and partial visibility when objects move out of the frame. For example, the appearance of a banana changes depending on the viewpoint, and a bottle becomes partially visible.
<hr>

<!-- ## Architecture -->
<div align="center"><h2> Architecture </h2></div>

![Overview](/static/image/Overview.png)
<!-- > <div align="center"><h4>Overview</h4></div> -->
Given a video and a query image, we extract feature vectors using a pre-trained visual encoder. We feed the feature vectors through a spatial decoder, followed by a temporal module and a prediction head outputs per-frame bounding boxes and scores.

<div align="center"><h4>Top-down Attention Guidance</h4></div>
![TAG](/static/image/TAG.png)
TAG guides the spatial decoderâ€™s attention using a high-level cue to capture the overall query context and a mid-level cue to enhance the understanding of fine-grained object parts.

<div align="center"><h4>Egocentric Augmentation based Consistency Training</h4></div>
![EgoACT](/static/image/EgoACT.png)
To enable robust matching, we replace the query image with a randomly selected corresponding object instance from the ground-truth. We reorder video frames based on object movement magnitude to simulate abrupt viewpoint changes. We also enforce temporal consistency, improving localization stability in egocentric videos.

<hr>

<!-- ## Experiments -->
<div align="center"><h2> Experiments </h2></div>

<div align="center">
<h4>Comparison with state-of-the-art on VQ2D</h4>
We report the tAP<sub>25</sub>, stAP<sub>25</sub>, recovery (Rec. %), and the success rate (Succ.) on the validation set and the test set.
</div>

![SOTA](/static/image/sota.png)


<div align="center">
<h4>Ablation study</h4>
To validate the effect of each component, we show the results on the VQ2D validation set. We report tAP<sub>25</sub>, stAP<sub>25</sub>, recovery (Rec. %), and success rate (Succ.) as percentages. 
</div>

![Ablation Study](/static/image/ablation_study.png)

<hr>

<div align="center"><h2> Visualization of TAG </h2></div>

![TAG](/static/image/tag_main.jpg)
We show the query image and four frames from each video with TAG's high-level and mid-level attention guides. The high-level attention guide emphasize the region of the target object and the mid-level attention guide attends to specific object parts in the query feature. They improve the model's ability to localize objects accurately, under varying perspectives or partial visibility.


<hr>

## Citation
```
@inproceedings{chang2025herovql,
  title={HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization},
  author={Chang, Joohyun and Hong, Soyeon and Lee, Hyogun and Ha, Seong Jong and Lee, Dongho and Kim, Seong Tae and Choi, Jinwoo},
  booktitle={The Thirty Sixth British Machine Vision Conference},
  year={2025},
}
```
